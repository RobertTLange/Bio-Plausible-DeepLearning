{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNS - Biological Plausible Deep Learning\n",
    "## Simple Backprop Baselines (DNNs and CNNs)\n",
    "\n",
    "#### 1. Data Setup - Download and Loading-In\n",
    "#### 2. PyTorch DNNs with Bayesian Optimization\n",
    "#### 3. PyTorch CNNs with Bayesian Optimization\n",
    "#### 4. Run Example 784/3072 - 500 - 10 Architecture on all Datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Packages\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "# Import tf for tensorboard monitoring of training\n",
    "import tensorflow as tf\n",
    "\n",
    "# Import Network Architectures\n",
    "from models.DNN import DNN, eval_dnn\n",
    "from models.CNN import CNN, eval_cnn\n",
    "\n",
    "# Import log-helper/learning plot functions\n",
    "from utils.helpers import *\n",
    "from utils.logger import *\n",
    "\n",
    "# Import Bayesian Optimization Module\n",
    "from utils.bayesian_opt import BO_NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch Device: Local CPU\n"
     ]
    }
   ],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Torch Device: {}\".format(torch.cuda.get_device_name(0)))\n",
    "else:\n",
    "    print(\"Torch Device: Local CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted Old TF/TensorBoard Log Files in Existing Log Directory\n"
     ]
    }
   ],
   "source": [
    "# Create all necessary directory if non-existent\n",
    "global data_dir\n",
    "data_dir = os.getcwd() +\"/data\"\n",
    "\n",
    "if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)\n",
    "    print(\"Created New Data Directory\")\n",
    "\n",
    "# Create Log Directory or remove tensorboard log files in log dir\n",
    "log_dir = os.getcwd() + \"/logs\"\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "    print(\"Created New Log Directory\")\n",
    "else:\n",
    "    filelist = [ f for f in os.listdir(log_dir) if f.startswith(\"events\")]\n",
    "    for f in filelist:\n",
    "        os.remove(os.path.join(log_dir, f))\n",
    "    print(\"Deleted Old TF/TensorBoard Log Files in Existing Log Directory\")\n",
    "    \n",
    "models_dir = os.getcwd() + \"/models\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download, Import and Plot Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No download of MNIST needed.\n",
      "No download of Fashion-MNIST needed.\n",
      "No download of CIFAR-10 needed.\n"
     ]
    }
   ],
   "source": [
    "download_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST dataset\n",
    "X_mnist, y_mnist = get_data(num_samples=70000, dataset=\"mnist\")\n",
    "# MNIST dataset\n",
    "X_fashion, y_fashion = get_data(num_samples=70000, dataset=\"fashion\")\n",
    "# MNIST dataset\n",
    "X_cifar10, y_cifar10 = get_data(num_samples=60000, dataset=\"cifar10\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Feedforward Neural Net\n",
    "\n",
    "### Run a Simple DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_mnist, y_mnist,\n",
    "                                                    stratify=y_mnist,\n",
    "                                                    random_state=0)\n",
    "\n",
    "# Define batchsize for data-loading/Epochs for training\n",
    "batch_size = 100\n",
    "num_epochs = 5\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Instantiate the model with layersize and Logging directory\n",
    "dnn_model = DNN(h_sizes=[784, 500], out_size=10)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(dnn_model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train| epoch  1| batch 20000/41996| acc: 0.9894| loss: 0.0358| time: 0.29\n",
      "valid| epoch  1| batch 20000/41996| acc: 0.9724| loss: 0.0917| time: 0.08\n",
      "-------------------------------------------------------------------------\n",
      "train| epoch  1| batch 40000/41996| acc: 0.9916| loss: 0.0301| time: 0.29\n",
      "valid| epoch  1| batch 40000/41996| acc: 0.9727| loss: 0.0912| time: 0.10\n",
      "-------------------------------------------------------------------------\n",
      "train| epoch  2| batch 20000/41996| acc: 0.9921| loss: 0.0277| time: 0.29\n",
      "valid| epoch  2| batch 20000/41996| acc: 0.9735| loss: 0.0895| time: 0.09\n",
      "-------------------------------------------------------------------------\n",
      "train| epoch  2| batch 40000/41996| acc: 0.9940| loss: 0.0235| time: 0.29\n",
      "valid| epoch  2| batch 40000/41996| acc: 0.9729| loss: 0.0911| time: 0.08\n",
      "-------------------------------------------------------------------------\n",
      "train| epoch  3| batch 20000/41996| acc: 0.9943| loss: 0.0203| time: 0.31\n",
      "valid| epoch  3| batch 20000/41996| acc: 0.9743| loss: 0.0879| time: 0.08\n",
      "-------------------------------------------------------------------------\n",
      "train| epoch  3| batch 40000/41996| acc: 0.9958| loss: 0.0179| time: 0.31\n",
      "valid| epoch  3| batch 40000/41996| acc: 0.9730| loss: 0.0908| time: 0.09\n",
      "-------------------------------------------------------------------------\n",
      "train| epoch  4| batch 20000/41996| acc: 0.9956| loss: 0.0161| time: 0.29\n",
      "valid| epoch  4| batch 20000/41996| acc: 0.9751| loss: 0.0892| time: 0.08\n",
      "-------------------------------------------------------------------------\n",
      "train| epoch  4| batch 40000/41996| acc: 0.9961| loss: 0.0148| time: 0.29\n",
      "valid| epoch  4| batch 40000/41996| acc: 0.9733| loss: 0.0929| time: 0.07\n",
      "-------------------------------------------------------------------------\n",
      "train| epoch  5| batch 20000/41996| acc: 0.9959| loss: 0.0143| time: 0.29\n",
      "valid| epoch  5| batch 20000/41996| acc: 0.9747| loss: 0.0931| time: 0.09\n",
      "-------------------------------------------------------------------------\n",
      "train| epoch  5| batch 40000/41996| acc: 0.9959| loss: 0.0140| time: 0.28\n",
      "valid| epoch  5| batch 40000/41996| acc: 0.9739| loss: 0.0976| time: 0.08\n",
      "-------------------------------------------------------------------------\n",
      "Test Accuracy: 0.9792\n"
     ]
    }
   ],
   "source": [
    "model = train_model(\"dnn\", dnn_model, num_epochs,\n",
    "                    X_train, y_train, batch_size,\n",
    "                    device, optimizer, criterion,\n",
    "                    log_freq = 20000,\n",
    "                    model_fname =\"models/temp_model_dnn_mnist.ckpt\",\n",
    "                    verbose=True, logging=True)\n",
    "\n",
    "# Get test error\n",
    "score = get_test_error(\"dnn\", device, model, X_test, y_test)\n",
    "print(\"Test Accuracy: {}\".format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Cross Validation Accuracy for all 3 Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: mnist\n",
      "Batchsize: 100\n",
      "Learning Rate: 0.001\n",
      "Architecture of Cross-Validated Network:\n",
      "\t Layer 0: 784 Units\n",
      "\t Layer 1: 500 Units\n",
      "Cross-Validation Score Fold 1: 0.9694892012341446\n",
      "Cross-Validation Score Fold 2: 0.9687995542793468\n",
      "Cross-Validation Score Fold 3: 0.975011786892975\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9711001808021554"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run 3-fold cross-validation on specific architecture for MNIST\n",
    "eval_dnn(\"mnist\", batch_size, learning_rate,\n",
    "         num_layers=1, h_l_1=500,\n",
    "         num_epochs=5, k_fold=3, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: fashion\n",
      "Batchsize: 100\n",
      "Learning Rate: 0.001\n",
      "Architecture of Cross-Validated Network:\n",
      "\t Layer 0: 784 Units\n",
      "\t Layer 1: 500 Units\n",
      "Cross-Validation Score Fold 1: 0.876092544987\n",
      "Cross-Validation Score Fold 2: 0.877111015859\n",
      "Cross-Validation Score Fold 3: 0.758551221603\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8372515941498805"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run 3-fold cross-validation on specific architecture for Fashion-MNIST\n",
    "eval_dnn(\"fashion\", batch_size, learning_rate,\n",
    "         num_layers=1, h_l_1=500,\n",
    "         num_epochs=5, k_fold=3, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: cifar10\n",
      "Batchsize: 100\n",
      "Learning Rate: 0.001\n",
      "Architecture of Cross-Validated Network:\n",
      "\t Layer 0: 3072 Units\n",
      "\t Layer 1: 500 Units\n",
      "Cross-Validation Score Fold 1: 0.4278\n",
      "Cross-Validation Score Fold 2: 0.434\n",
      "Cross-Validation Score Fold 3: 0.40585\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.42255"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run 3-fold cross-validation on specific architecture for CIFAR-10\n",
    "eval_dnn(\"cifar10\", batch_size, learning_rate,\n",
    "         num_layers=1, h_l_1=500,\n",
    "         num_epochs=5, k_fold=3, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Bayesian Optimization on DNN Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Search Hyperspace for Bayesian Optimization on DNN architectures\n",
    "hyper_space_dnn = {'batch_size': (10, 500),\n",
    "                   'learning_rate': (0.0001, 0.05),\n",
    "                   'num_layers': (1, 6),\n",
    "                   'h_l_1': (30, 500),\n",
    "                   'h_l_2': (30, 500),\n",
    "                   'h_l_3': (30, 500),\n",
    "                   'h_l_4': (30, 500),\n",
    "                   'h_l_5': (30, 500),\n",
    "                   'h_l_6': (30, 500)}\n",
    "\n",
    "bo_iters = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Logging to ./logs/bo_logs_dnn_mnist.json\n",
      "BO iter  1 | cv-acc: 0.9590 | best-acc: 0.9590 | time: 29.70\n",
      "BO iter  2 | cv-acc: 0.9629 | best-acc: 0.9629 | time: 38.18\n",
      "BO iter  3 | cv-acc: 0.9004 | best-acc: 0.9629 | time: 31.98\n",
      "BO iter  4 | cv-acc: 0.1361 | best-acc: 0.9629 | time: 122.09\n",
      "BO iter  5 | cv-acc: 0.9315 | best-acc: 0.9629 | time: 39.83\n",
      "BO iter  6 | cv-acc: 0.5563 | best-acc: 0.9629 | time: 238.18\n",
      "BO iter  7 | cv-acc: 0.8754 | best-acc: 0.9629 | time: 34.68\n",
      "BO iter  8 | cv-acc: 0.9472 | best-acc: 0.9629 | time: 37.67\n",
      "BO iter  9 | cv-acc: 0.8748 | best-acc: 0.9629 | time: 36.25\n",
      "BO iter 10 | cv-acc: 0.9255 | best-acc: 0.9629 | time: 40.50\n",
      "BO iter 11 | cv-acc: 0.9371 | best-acc: 0.9629 | time: 33.72\n",
      "BO iter 12 | cv-acc: 0.6745 | best-acc: 0.9629 | time: 37.56\n",
      "BO iter 13 | cv-acc: 0.9253 | best-acc: 0.9629 | time: 33.46\n",
      "BO iter 14 | cv-acc: 0.9496 | best-acc: 0.9629 | time: 44.18\n",
      "BO iter 15 | cv-acc: 0.9284 | best-acc: 0.9629 | time: 41.98\n",
      "BO iter 16 | cv-acc: 0.9738 | best-acc: 0.9738 | time: 231.82\n",
      "BO iter 17 | cv-acc: 0.9741 | best-acc: 0.9741 | time: 230.70\n",
      "BO iter 18 | cv-acc: 0.9453 | best-acc: 0.9741 | time: 43.87\n",
      "BO iter 19 | cv-acc: 0.9361 | best-acc: 0.9741 | time: 227.48\n",
      "BO iter 20 | cv-acc: 0.9372 | best-acc: 0.9741 | time: 229.59\n",
      "BO iter 21 | cv-acc: 0.9252 | best-acc: 0.9741 | time: 35.29\n",
      "BO iter 22 | cv-acc: 0.9252 | best-acc: 0.9741 | time: 35.97\n",
      "BO iter 23 | cv-acc: 0.8769 | best-acc: 0.9741 | time: 36.85\n",
      "BO iter 24 | cv-acc: 0.9610 | best-acc: 0.9741 | time: 125.39\n",
      "BO iter 25 | cv-acc: 0.9409 | best-acc: 0.9741 | time: 52.10\n",
      "BO iter 26 | cv-acc: 0.9474 | best-acc: 0.9741 | time: 51.63\n",
      "BO iter 27 | cv-acc: 0.0986 | best-acc: 0.9741 | time: 553.48\n",
      "BO iter 28 | cv-acc: 0.8892 | best-acc: 0.9741 | time: 37.26\n",
      "BO iter 29 | cv-acc: 0.9253 | best-acc: 0.9741 | time: 32.41\n",
      "BO iter 30 | cv-acc: 0.9610 | best-acc: 0.9741 | time: 543.09\n",
      "BO iter 31 | cv-acc: 0.9403 | best-acc: 0.9741 | time: 45.21\n",
      "BO iter 32 | cv-acc: 0.8751 | best-acc: 0.9741 | time: 30.30\n",
      "BO iter 33 | cv-acc: 0.9255 | best-acc: 0.9741 | time: 32.70\n",
      "BO iter 34 | cv-acc: 0.9246 | best-acc: 0.9741 | time: 35.12\n",
      "BO iter 35 | cv-acc: 0.9274 | best-acc: 0.9741 | time: 45.65\n",
      "BO iter 36 | cv-acc: 0.9200 | best-acc: 0.9741 | time: 46.93\n",
      "BO iter 37 | cv-acc: 0.9363 | best-acc: 0.9741 | time: 234.68\n",
      "BO iter 38 | cv-acc: 0.9375 | best-acc: 0.9741 | time: 232.51\n",
      "BO iter 39 | cv-acc: 0.9732 | best-acc: 0.9741 | time: 237.07\n",
      "BO iter 40 | cv-acc: 0.9452 | best-acc: 0.9741 | time: 550.78\n",
      "BO iter 41 | cv-acc: 0.9722 | best-acc: 0.9741 | time: 237.74\n",
      "BO iter 42 | cv-acc: 0.9367 | best-acc: 0.9741 | time: 236.17\n",
      "BO iter 43 | cv-acc: 0.9729 | best-acc: 0.9741 | time: 240.00\n",
      "BO iter 44 | cv-acc: 0.9692 | best-acc: 0.9741 | time: 559.10\n",
      "BO iter 45 | cv-acc: 0.9359 | best-acc: 0.9741 | time: 239.32\n",
      "BO iter 46 | cv-acc: 0.9356 | best-acc: 0.9741 | time: 237.92\n",
      "BO iter 47 | cv-acc: 0.9118 | best-acc: 0.9741 | time: 42.94\n",
      "BO iter 48 | cv-acc: 0.9076 | best-acc: 0.9741 | time: 42.31\n",
      "BO iter 49 | cv-acc: 0.9244 | best-acc: 0.9741 | time: 46.57\n",
      "BO iter 50 | cv-acc: 0.9249 | best-acc: 0.9741 | time: 45.37\n"
     ]
    }
   ],
   "source": [
    "# Run Bayesian Optimization (UCB-Acquisition Fct) on DNN for MNIST\n",
    "opt_log = BO_NN(bo_iters, eval_dnn, \"dnn\", \"mnist\", hyper_space_dnn,\n",
    "                num_epochs=10, k_fold=3, logging=True, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded previously existing Log with 51 BO iterations.\n",
      "Start Logging to ./logs/bo_logs_dnn_mnist.json\n"
     ]
    }
   ],
   "source": [
    "# Run Bayesian Optimization (UCB-Acquisition Fct) on DNN for MNIST\n",
    "opt_log = BO_NN(2, eval_dnn, \"dnn\", \"mnist\", hyper_space_dnn,\n",
    "                num_epochs=2, k_fold=3, logging=True, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Logging to ./logs/bo_logs_dnn_fashion.json\n",
      "BO iter  1 | cv-acc: 0.8530 | best-acc: 0.8530 | time: 30.22\n",
      "BO iter  2 | cv-acc: 0.8560 | best-acc: 0.8560 | time: 38.44\n",
      "BO iter  3 | cv-acc: 0.6448 | best-acc: 0.8560 | time: 32.25\n",
      "BO iter  4 | cv-acc: 0.8574 | best-acc: 0.8574 | time: 545.27\n",
      "BO iter  5 | cv-acc: 0.2121 | best-acc: 0.8574 | time: 232.47\n",
      "BO iter  6 | cv-acc: 0.8607 | best-acc: 0.8607 | time: 554.57\n",
      "BO iter  7 | cv-acc: 0.7732 | best-acc: 0.8607 | time: 48.55\n",
      "BO iter  8 | cv-acc: 0.8727 | best-acc: 0.8727 | time: 555.34\n",
      "BO iter  9 | cv-acc: 0.7778 | best-acc: 0.8727 | time: 45.31\n",
      "BO iter 10 | cv-acc: 0.8319 | best-acc: 0.8727 | time: 35.15\n",
      "BO iter 11 | cv-acc: 0.8430 | best-acc: 0.8727 | time: 34.44\n",
      "BO iter 12 | cv-acc: 0.1983 | best-acc: 0.8727 | time: 228.98\n",
      "BO iter 13 | cv-acc: 0.8796 | best-acc: 0.8796 | time: 552.15\n",
      "BO iter 14 | cv-acc: 0.8537 | best-acc: 0.8796 | time: 547.69\n",
      "BO iter 15 | cv-acc: 0.8263 | best-acc: 0.8796 | time: 40.06\n",
      "BO iter 16 | cv-acc: 0.7475 | best-acc: 0.8796 | time: 36.74\n",
      "BO iter 17 | cv-acc: 0.8265 | best-acc: 0.8796 | time: 46.92\n",
      "BO iter 18 | cv-acc: 0.8757 | best-acc: 0.8796 | time: 545.47\n",
      "BO iter 19 | cv-acc: 0.8664 | best-acc: 0.8796 | time: 546.57\n",
      "BO iter 20 | cv-acc: 0.8540 | best-acc: 0.8796 | time: 543.36\n",
      "BO iter 21 | cv-acc: 0.8464 | best-acc: 0.8796 | time: 546.13\n",
      "BO iter 22 | cv-acc: 0.8416 | best-acc: 0.8796 | time: 46.38\n",
      "BO iter 23 | cv-acc: 0.8126 | best-acc: 0.8796 | time: 45.55\n",
      "BO iter 24 | cv-acc: 0.3274 | best-acc: 0.8796 | time: 52.02\n",
      "BO iter 25 | cv-acc: 0.8113 | best-acc: 0.8796 | time: 38.89\n",
      "BO iter 26 | cv-acc: 0.8480 | best-acc: 0.8796 | time: 544.00\n",
      "BO iter 27 | cv-acc: 0.8080 | best-acc: 0.8796 | time: 39.35\n",
      "BO iter 28 | cv-acc: 0.8610 | best-acc: 0.8796 | time: 543.28\n",
      "BO iter 29 | cv-acc: 0.7266 | best-acc: 0.8796 | time: 35.98\n",
      "BO iter 30 | cv-acc: 0.8747 | best-acc: 0.8796 | time: 231.45\n",
      "BO iter 31 | cv-acc: 0.8508 | best-acc: 0.8796 | time: 546.96\n",
      "BO iter 32 | cv-acc: 0.8241 | best-acc: 0.8796 | time: 42.29\n",
      "BO iter 33 | cv-acc: 0.7838 | best-acc: 0.8796 | time: 35.20\n",
      "BO iter 34 | cv-acc: 0.8640 | best-acc: 0.8796 | time: 235.24\n",
      "BO iter 35 | cv-acc: 0.7888 | best-acc: 0.8796 | time: 45.44\n",
      "BO iter 36 | cv-acc: 0.7825 | best-acc: 0.8796 | time: 41.41\n",
      "BO iter 37 | cv-acc: 0.8641 | best-acc: 0.8796 | time: 243.01\n",
      "BO iter 38 | cv-acc: 0.8385 | best-acc: 0.8796 | time: 550.44\n",
      "BO iter 39 | cv-acc: 0.8412 | best-acc: 0.8796 | time: 53.14\n",
      "BO iter 40 | cv-acc: 0.8436 | best-acc: 0.8796 | time: 43.35\n",
      "BO iter 41 | cv-acc: 0.8733 | best-acc: 0.8796 | time: 244.62\n",
      "BO iter 42 | cv-acc: 0.1000 | best-acc: 0.8796 | time: 51.03\n",
      "BO iter 43 | cv-acc: 0.8420 | best-acc: 0.8796 | time: 543.73\n",
      "BO iter 44 | cv-acc: 0.8295 | best-acc: 0.8796 | time: 48.20\n",
      "BO iter 45 | cv-acc: 0.7647 | best-acc: 0.8796 | time: 41.54\n",
      "BO iter 46 | cv-acc: 0.8493 | best-acc: 0.8796 | time: 554.30\n",
      "BO iter 47 | cv-acc: 0.1000 | best-acc: 0.8796 | time: 549.32\n",
      "BO iter 48 | cv-acc: 0.8460 | best-acc: 0.8796 | time: 546.76\n",
      "BO iter 49 | cv-acc: 0.7955 | best-acc: 0.8796 | time: 41.34\n",
      "BO iter 50 | cv-acc: 0.8599 | best-acc: 0.8796 | time: 546.03\n"
     ]
    }
   ],
   "source": [
    "# Run Bayesian Optimization (UCB-Acquisition Fct) on DNN for Fashion-MNIST\n",
    "opt_log = BO_NN(bo_iters, eval_dnn, \"dnn\", \"fashion\", hyper_space_dnn,\n",
    "                num_epochs=10, k_fold=3, logging=True, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Logging to ./logs/bo_logs_dnn_cifar10.json\n",
      "BO iter  1 | cv-acc: 0.1000 | best-acc: 0.1000 | time: 42.11\n",
      "BO iter  2 | cv-acc: 0.2539 | best-acc: 0.2539 | time: 44.48\n",
      "BO iter  3 | cv-acc: 0.1000 | best-acc: 0.2539 | time: 32.56\n",
      "BO iter  4 | cv-acc: 0.4222 | best-acc: 0.4222 | time: 491.41\n",
      "BO iter  5 | cv-acc: 0.4090 | best-acc: 0.4222 | time: 483.14\n",
      "BO iter  6 | cv-acc: 0.4132 | best-acc: 0.4222 | time: 491.96\n",
      "BO iter  7 | cv-acc: 0.1000 | best-acc: 0.4222 | time: 216.67\n",
      "BO iter  8 | cv-acc: 0.4533 | best-acc: 0.4533 | time: 512.72\n",
      "BO iter  9 | cv-acc: 0.3673 | best-acc: 0.4533 | time: 56.29\n",
      "BO iter 10 | cv-acc: 0.3913 | best-acc: 0.4533 | time: 486.15\n",
      "BO iter 11 | cv-acc: 0.4206 | best-acc: 0.4533 | time: 488.19\n",
      "BO iter 12 | cv-acc: 0.1000 | best-acc: 0.4533 | time: 288.32\n",
      "BO iter 13 | cv-acc: 0.3621 | best-acc: 0.4533 | time: 62.50\n",
      "BO iter 14 | cv-acc: 0.4229 | best-acc: 0.4533 | time: 491.41\n",
      "BO iter 15 | cv-acc: 0.3767 | best-acc: 0.4533 | time: 56.58\n",
      "BO iter 16 | cv-acc: 0.3145 | best-acc: 0.4533 | time: 48.89\n",
      "BO iter 17 | cv-acc: 0.4183 | best-acc: 0.4533 | time: 486.25\n",
      "BO iter 18 | cv-acc: 0.4219 | best-acc: 0.4533 | time: 483.73\n"
     ]
    }
   ],
   "source": [
    "# Run Bayesian Optimization (UCB-Acquisition Fct) on DNN for CIFAR-10\n",
    "opt_log = BO_NN(bo_iters, eval_dnn, \"dnn\", \"cifar10\", hyper_space_dnn,\n",
    "                num_epochs=10, k_fold=3, logging=True, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_mnist, y_mnist,\n",
    "                                                    stratify=y_mnist,\n",
    "                                                    random_state=0)\n",
    "\n",
    "# ConvNet Parameters\n",
    "batch_size = 100\n",
    "ch_sizes = [1, 16, 32]\n",
    "k_sizes = [5, 5]\n",
    "stride = 1\n",
    "padding = 2\n",
    "out_size = 10\n",
    "num_epochs = 2\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Instantiate the model with layersizes, Loss fct, optimizer\n",
    "cnn_model = CNN(ch_sizes, k_sizes,\n",
    "                stride, padding, out_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(cnn_model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = train_model(\"cnn\", cnn_model, num_epochs,\n",
    "                    X_train, y_train, batch_size,\n",
    "                    device, optimizer, criterion, log_freq=10000,\n",
    "                    model_fname =\"models/temp_model_cnn.ckpt\",\n",
    "                    verbose=False, logging=True)\n",
    "\n",
    "# Get test error\n",
    "score = get_test_error(\"cnn\", device, model, X_test, y_test)\n",
    "print(\"Test Accuracy: {}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_fname = get_latest_log_fname(log_dir)\n",
    "its, train_loss, val_loss, train_acc, val_acc = process_logger(log_fname)\n",
    "plot_learning(its, train_acc, val_acc, train_loss, val_loss, \"CNN - Learning Performance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run 3-fold cross-validation on specific architecture\n",
    "eval_cnn(batch_size, learning_rate, num_layers=2,\n",
    "         ch_1=16, ch_2=32, k_1=5, k_2=5,\n",
    "         stride=1, padding=2,\n",
    "         k_fold=2, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Bayesian Optimization (UCB-Acquisition Fct) on DNN\n",
    "hyper_space_cnn = {'batch_size': (10, 500),\n",
    "                   'learning_rate': (0.0001, 0.05),\n",
    "                   'num_layers': (1, 5),\n",
    "                   'ch_1': (3, 64),\n",
    "                   'ch_2': (3, 64),\n",
    "                   'ch_3': (3, 64),\n",
    "                   'ch_4': (3, 64),\n",
    "                   'ch_5': (3, 64),\n",
    "                   'k_1': (2, 10),\n",
    "                   'k_2': (2, 10),\n",
    "                   'k_3': (2, 10),\n",
    "                   'k_4': (2, 10),\n",
    "                   'k_5': (2, 10),\n",
    "                   'stride': (1, 3),\n",
    "                   'padding': (1, 3)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Bayesian Optimization (UCB-Acquisition Fct) on CNN for MNIST\n",
    "opt_log = BO_NN(bo_iters, eval_cnn, \"cnn\", \"mnist\", hyper_space_cnn,\n",
    "                num_epochs=10, k_fold=3, logging=True, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Bayesian Optimization (UCB-Acquisition Fct) on CNN for Fashion-MNIST\n",
    "opt_log = BO_NN(bo_iters, eval_cnn, \"cnn\", \"fashion\", hyper_space_cnn,\n",
    "                num_epochs=10, k_fold=3, logging=True, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Bayesian Optimization (UCB-Acquisition Fct) on DNN for CIFAR-10\n",
    "opt_log = BO_NN(bo_iters, eval_cnn, \"cnn\", \"cifar10\", hyper_space_dnn,\n",
    "                num_epochs=10, k_fold=3, logging=True, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Full Models (10 Epochs, Logging every 5000 eps and all Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_mnist, y_mnist,\n",
    "                                                    stratify=y_mnist,\n",
    "                                                    random_state=0)\n",
    "\n",
    "# Define batchsize for data-loading/Epochs for training\n",
    "batch_size = 100\n",
    "num_epochs = 10\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Instantiate the model with layersize and Logging directory\n",
    "dnn_model = DNN(h_sizes=[784, 500], out_size=10)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(dnn_model.parameters(), lr=learning_rate)\n",
    "\n",
    "model = train_model(\"dnn\", dnn_model, 10,\n",
    "                    X_train, y_train, batch_size,\n",
    "                    device, optimizer, criterion,\n",
    "                    log_freq = 5000,\n",
    "                    model_fname =\"models/temp_model_dnn_mnist.ckpt\",\n",
    "                    verbose=False, logging=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_fashion, y_fashion,\n",
    "                                                    stratify=y_fashion,\n",
    "                                                    random_state=0)\n",
    "\n",
    "# Instantiate the model with layersize and Logging directory\n",
    "dnn_model = DNN(h_sizes=[784, 500], out_size=10)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(dnn_model.parameters(), lr=learning_rate)\n",
    "\n",
    "model = train_model(\"dnn\", dnn_model, 10,\n",
    "                    X_train, y_train, batch_size,\n",
    "                    device, optimizer, criterion,\n",
    "                    log_freq = 5000,\n",
    "                    model_fname =\"models/temp_model_dnn_fashion.ckpt\",\n",
    "                    verbose=False, logging=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_cifar10, y_cifar10,\n",
    "                                                    stratify=y_cifar10,\n",
    "                                                    random_state=0)\n",
    "\n",
    "# Instantiate the model with layersize and Logging directory\n",
    "dnn_model = DNN(h_sizes=[3072, 500], out_size=10)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(dnn_model.parameters(), lr=learning_rate)\n",
    "\n",
    "model = train_model(\"dnn\", dnn_model, 10,\n",
    "                    X_train, y_train, batch_size,\n",
    "                    device, optimizer, criterion,\n",
    "                    log_freq = 5000,\n",
    "                    model_fname =\"models/temp_model_dnn_cifar.ckpt\",\n",
    "                    verbose=False, logging=True)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python3 (BioDL)",
   "language": "python",
   "name": "biodl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
