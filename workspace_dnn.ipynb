{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNS - Biological Plausible Deep Learning\n",
    "## Simple Backprop Baselines (DNNs and CNNs)\n",
    "\n",
    "#### 1. Data Setup - Download and Loading-In\n",
    "#### 2. PyTorch DNNs with Bayesian Optimization\n",
    "#### 3. PyTorch CNNs with Bayesian Optimization\n",
    "#### 4. Run Example 784/3072 - 500 - 10 Architecture on all Datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Packages\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "# Import tf for tensorboard monitoring of training\n",
    "import tensorflow as tf\n",
    "\n",
    "# Import Network Architectures\n",
    "from DNN import DNN, eval_dnn\n",
    "from CNN import CNN, eval_cnn\n",
    "\n",
    "# Import log-helper/learning plot functions\n",
    "from helpers import *\n",
    "from logger import *\n",
    "\n",
    "# Import Bayesian Optimization Module\n",
    "from bayesian_opt import BO_NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch Device: Local CPU\n"
     ]
    }
   ],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Torch Device: {}\".format(torch.cuda.get_device_name(0)))\n",
    "else:\n",
    "    print(\"Torch Device: Local CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted Old TF/TensorBoard Log Files in Existing Log Directory\n"
     ]
    }
   ],
   "source": [
    "# Create all necessary directory if non-existent\n",
    "data_dir = os.getcwd() +\"/data\"\n",
    "global data_dir\n",
    "\n",
    "if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)\n",
    "    print(\"Created New Data Directory\")\n",
    "\n",
    "# Create Log Directory or remove tensorboard log files in log dir\n",
    "log_dir = os.getcwd() + \"/logs\"\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "    print(\"Created New Log Directory\")\n",
    "else:\n",
    "    filelist = [ f for f in os.listdir(log_dir) if f.startswith(\"events\")]\n",
    "    for f in filelist:\n",
    "        os.remove(os.path.join(log_dir, f))\n",
    "    print(\"Deleted Old TF/TensorBoard Log Files in Existing Log Directory\")\n",
    "    \n",
    "models_dir = os.getcwd() + \"/models\"\n",
    "if not os.path.exists(models_dir):\n",
    "    os.makedirs(models_dir)\n",
    "    print(\"Created New Model Directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download, Import and Plot Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No download of MNIST needed.\n",
      "No download of Fashion-MNIST needed.\n",
      "No download of CIFAR-10 needed.\n"
     ]
    }
   ],
   "source": [
    "download_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST dataset\n",
    "X_mnist, y_mnist = get_data(num_samples=70000, dataset=\"mnist\")\n",
    "# MNIST dataset\n",
    "X_fashion, y_fashion = get_data(num_samples=70000, dataset=\"fashion\")\n",
    "# MNIST dataset\n",
    "X_cifar10, y_cifar10 = get_data(num_samples=60000, dataset=\"cifar10\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Feedforward Neural Net\n",
    "\n",
    "### Run a Simple DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_mnist, y_mnist,\n",
    "                                                    stratify=y_mnist,\n",
    "                                                    random_state=0)\n",
    "\n",
    "# Define batchsize for data-loading/Epochs for training\n",
    "batch_size = 100\n",
    "num_epochs = 5\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Instantiate the model with layersize and Logging directory\n",
    "dnn_model = DNN(h_sizes=[784, 500], out_size=10)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(dnn_model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train| epoch  1| batch 20000/41996| acc: 0.9738| loss: 0.0992| time: 0.31\n",
      "valid| epoch  1| batch 20000/41996| acc: 0.9532| loss: 0.2329| time: 0.08\n",
      "-------------------------------------------------------------------------\n",
      "train| epoch  1| batch 40000/41996| acc: 0.9605| loss: 0.1577| time: 0.30\n",
      "valid| epoch  1| batch 40000/41996| acc: 0.9423| loss: 0.2971| time: 0.10\n",
      "-------------------------------------------------------------------------\n",
      "train| epoch  2| batch 20000/41996| acc: 0.9735| loss: 0.1118| time: 0.31\n",
      "valid| epoch  2| batch 20000/41996| acc: 0.9549| loss: 0.2584| time: 0.09\n",
      "-------------------------------------------------------------------------\n",
      "train| epoch  2| batch 40000/41996| acc: 0.9748| loss: 0.0999| time: 0.37\n",
      "valid| epoch  2| batch 40000/41996| acc: 0.9554| loss: 0.2396| time: 0.11\n",
      "-------------------------------------------------------------------------\n",
      "train| epoch  3| batch 20000/41996| acc: 0.9745| loss: 0.1028| time: 0.30\n",
      "valid| epoch  3| batch 20000/41996| acc: 0.9542| loss: 0.2608| time: 0.09\n",
      "-------------------------------------------------------------------------\n",
      "train| epoch  3| batch 40000/41996| acc: 0.9776| loss: 0.0962| time: 0.33\n",
      "valid| epoch  3| batch 40000/41996| acc: 0.9601| loss: 0.2403| time: 0.09\n",
      "-------------------------------------------------------------------------\n",
      "train| epoch  4| batch 20000/41996| acc: 0.9793| loss: 0.0788| time: 0.32\n",
      "valid| epoch  4| batch 20000/41996| acc: 0.9577| loss: 0.2351| time: 0.08\n",
      "-------------------------------------------------------------------------\n",
      "train| epoch  4| batch 40000/41996| acc: 0.9725| loss: 0.1335| time: 0.31\n",
      "valid| epoch  4| batch 40000/41996| acc: 0.9575| loss: 0.2872| time: 0.10\n",
      "-------------------------------------------------------------------------\n",
      "train| epoch  5| batch 20000/41996| acc: 0.9765| loss: 0.1130| time: 0.32\n",
      "valid| epoch  5| batch 20000/41996| acc: 0.9579| loss: 0.3269| time: 0.09\n",
      "-------------------------------------------------------------------------\n",
      "train| epoch  5| batch 40000/41996| acc: 0.9750| loss: 0.1303| time: 0.36\n",
      "valid| epoch  5| batch 40000/41996| acc: 0.9561| loss: 0.3547| time: 0.09\n",
      "-------------------------------------------------------------------------\n",
      "train| epoch  6| batch 20000/41996| acc: 0.9749| loss: 0.1426| time: 0.33\n",
      "valid| epoch  6| batch 20000/41996| acc: 0.9549| loss: 0.3873| time: 0.10\n",
      "-------------------------------------------------------------------------\n",
      "train| epoch  6| batch 40000/41996| acc: 0.9707| loss: 0.1725| time: 0.29\n",
      "valid| epoch  6| batch 40000/41996| acc: 0.9518| loss: 0.4101| time: 0.09\n",
      "-------------------------------------------------------------------------\n",
      "train| epoch  7| batch 20000/41996| acc: 0.9825| loss: 0.0814| time: 0.35\n",
      "valid| epoch  7| batch 20000/41996| acc: 0.9601| loss: 0.3243| time: 0.12\n",
      "-------------------------------------------------------------------------\n",
      "train| epoch  7| batch 40000/41996| acc: 0.9790| loss: 0.1015| time: 0.30\n",
      "valid| epoch  7| batch 40000/41996| acc: 0.9594| loss: 0.3335| time: 0.10\n",
      "-------------------------------------------------------------------------\n",
      "train| epoch  8| batch 20000/41996| acc: 0.9787| loss: 0.0995| time: 0.42\n",
      "valid| epoch  8| batch 20000/41996| acc: 0.9592| loss: 0.3241| time: 0.11\n",
      "-------------------------------------------------------------------------\n",
      "train| epoch  8| batch 40000/41996| acc: 0.9811| loss: 0.1072| time: 0.29\n",
      "valid| epoch  8| batch 40000/41996| acc: 0.9604| loss: 0.3770| time: 0.10\n",
      "-------------------------------------------------------------------------\n",
      "train| epoch  9| batch 20000/41996| acc: 0.9803| loss: 0.1019| time: 0.35\n",
      "valid| epoch  9| batch 20000/41996| acc: 0.9610| loss: 0.3499| time: 0.09\n",
      "-------------------------------------------------------------------------\n",
      "train| epoch  9| batch 40000/41996| acc: 0.9760| loss: 0.1958| time: 0.32\n",
      "valid| epoch  9| batch 40000/41996| acc: 0.9582| loss: 0.4995| time: 0.10\n",
      "-------------------------------------------------------------------------\n",
      "train| epoch 10| batch 20000/41996| acc: 0.9804| loss: 0.1210| time: 0.54\n",
      "valid| epoch 10| batch 20000/41996| acc: 0.9626| loss: 0.4084| time: 0.12\n",
      "-------------------------------------------------------------------------\n",
      "train| epoch 10| batch 40000/41996| acc: 0.9836| loss: 0.0950| time: 0.32\n",
      "valid| epoch 10| batch 40000/41996| acc: 0.9650| loss: 0.3904| time: 0.08\n",
      "-------------------------------------------------------------------------\n",
      "Test Accuracy: 0.962914285714\n"
     ]
    }
   ],
   "source": [
    "model = train_model(\"dnn\", dnn_model, num_epochs,\n",
    "                    X_train, y_train, batch_size,\n",
    "                    device, optimizer, criterion,\n",
    "                    log_freq = 20000,\n",
    "                    model_fname =\"models/temp_model_dnn_mnist.ckpt\",\n",
    "                    verbose=True, logging=True)\n",
    "\n",
    "# Get test error\n",
    "score = get_test_error(\"dnn\", device, model, X_test, y_test)\n",
    "print(\"Test Accuracy: {}\".format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Cross Validation Accuracy for all 3 Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run 3-fold cross-validation on specific architecture for MNIST\n",
    "eval_dnn(\"mnist\", batch_size, learning_rate,\n",
    "         num_layers=1, h_l_1=500,\n",
    "         num_epochs=5, k_fold=3, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run 3-fold cross-validation on specific architecture for Fashion-MNIST\n",
    "eval_dnn(\"fashion\", batch_size, learning_rate,\n",
    "         num_layers=1, h_l_1=500,\n",
    "         num_epochs=5, k_fold=3, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run 3-fold cross-validation on specific architecture for CIFAR-10\n",
    "eval_dnn(\"cifar10\", batch_size, learning_rate,\n",
    "         num_layers=1, h_l_1=500,\n",
    "         num_epochs=5, k_fold=3, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Bayesian Optimization on DNN Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Search Hyperspace for Bayesian Optimization on DNN architectures\n",
    "hyper_space_dnn = {'batch_size': (10, 500),\n",
    "                   'learning_rate': (0.0001, 0.05),\n",
    "                   'num_layers': (1, 6),\n",
    "                   'h_l_1': (30, 500),\n",
    "                   'h_l_2': (30, 500),\n",
    "                   'h_l_3': (30, 500),\n",
    "                   'h_l_4': (30, 500),\n",
    "                   'h_l_5': (30, 500),\n",
    "                   'h_l_6': (30, 500)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Bayesian Optimization (UCB-Acquisition Fct) on DNN for MNIST\n",
    "opt_log = BO_NN(2, eval_dnn, \"dnn\", \"mnist\", hyper_space_dnn,\n",
    "                num_epochs=10, k_fold=3, logging=True, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Bayesian Optimization (UCB-Acquisition Fct) on DNN for Fashion-MNIST\n",
    "opt_log = BO_NN(2, eval_dnn, \"dnn\", \"fashion\", hyper_space_dnn,\n",
    "                num_epochs=10, k_fold=3, logging=True, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Bayesian Optimization (UCB-Acquisition Fct) on DNN for CIFAR-10\n",
    "opt_log = BO_NN(2, eval_dnn, \"dnn\", \"cifar10\", hyper_space_dnn,\n",
    "                num_epochs=10, k_fold=3, logging=True, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ConvNet Parameters\n",
    "batch_size = 100\n",
    "ch_sizes = [1, 16, 32]\n",
    "k_sizes = [5, 5]\n",
    "stride = 1\n",
    "padding = 2\n",
    "out_size = 10\n",
    "num_epochs = 2\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Instantiate the model with layersizes, Loss fct, optimizer\n",
    "cnn_model = CNN(ch_sizes, k_sizes,\n",
    "                stride, padding, out_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(cnn_model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = train_model(\"cnn\", cnn_model, num_epochs,\n",
    "                    X_train, y_train, batch_size,\n",
    "                    device, optimizer, criterion, log_freq=10000,\n",
    "                    model_fname =\"models/temp_model_cnn.ckpt\",\n",
    "                    verbose=False, logging=True)\n",
    "\n",
    "# Get test error\n",
    "score = get_test_error(\"cnn\", device, model, X_test, y_test)\n",
    "print(\"Test Accuracy: {}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_fname = get_latest_log_fname(log_dir)\n",
    "its, train_loss, val_loss, train_acc, val_acc = process_logger(log_fname)\n",
    "plot_learning(its, train_acc, val_acc, train_loss, val_loss, \"CNN - Learning Performance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run 3-fold cross-validation on specific architecture\n",
    "eval_cnn(batch_size, learning_rate, num_layers=2,\n",
    "         ch_1=16, ch_2=32, k_1=5, k_2=5,\n",
    "         stride=1, padding=2,\n",
    "         k_fold=2, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Bayesian Optimization (UCB-Acquisition Fct) on DNN\n",
    "hyper_space_cnn = {'batch_size': (10, 500),\n",
    "                   'learning_rate': (0.0001, 0.05),\n",
    "                   'num_layers': (1, 5),\n",
    "                   'ch_1': (3, 64),\n",
    "                   'ch_2': (3, 64),\n",
    "                   'ch_3': (3, 64),\n",
    "                   'ch_4': (3, 64),\n",
    "                   'ch_5': (3, 64),\n",
    "                   'k_1': (2, 10),\n",
    "                   'k_2': (2, 10),\n",
    "                   'k_3': (2, 10),\n",
    "                   'k_4': (2, 10),\n",
    "                   'k_5': (2, 10),\n",
    "                   'stride': (1, 3),\n",
    "                   'padding': (1, 3)}\n",
    "\n",
    "opt_log = BO_NN(3, eval_cnn, \"cnn\", hyper_space_cnn, logging=False, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Full Models (10 Epochs, Logging every 5000 eps and all Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_mnist, y_mnist,\n",
    "                                                    stratify=y_mnist,\n",
    "                                                    random_state=0)\n",
    "\n",
    "# Define batchsize for data-loading/Epochs for training\n",
    "batch_size = 100\n",
    "num_epochs = 10\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Instantiate the model with layersize and Logging directory\n",
    "dnn_model = DNN(h_sizes=[784, 500], out_size=10)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(dnn_model.parameters(), lr=learning_rate)\n",
    "\n",
    "model = train_model(\"dnn\", dnn_model, 10,\n",
    "                    X_train, y_train, batch_size,\n",
    "                    device, optimizer, criterion,\n",
    "                    log_freq = 5000,\n",
    "                    model_fname =\"models/temp_model_dnn_mnist.ckpt\",\n",
    "                    verbose=False, logging=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_fashion, y_fashion,\n",
    "                                                    stratify=y_fashion,\n",
    "                                                    random_state=0)\n",
    "\n",
    "# Instantiate the model with layersize and Logging directory\n",
    "dnn_model = DNN(h_sizes=[784, 500], out_size=10)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(dnn_model.parameters(), lr=learning_rate)\n",
    "\n",
    "model = train_model(\"dnn\", dnn_model, 10,\n",
    "                    X_train, y_train, batch_size,\n",
    "                    device, optimizer, criterion,\n",
    "                    log_freq = 5000,\n",
    "                    model_fname =\"models/temp_model_dnn_fashion.ckpt\",\n",
    "                    verbose=False, logging=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_cifar10, y_cifar10,\n",
    "                                                    stratify=y_cifar10,\n",
    "                                                    random_state=0)\n",
    "\n",
    "# Instantiate the model with layersize and Logging directory\n",
    "dnn_model = DNN(h_sizes=[3072, 500], out_size=10)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(dnn_model.parameters(), lr=learning_rate)\n",
    "\n",
    "model = train_model(\"dnn\", dnn_model, 10,\n",
    "                    X_train, y_train, batch_size,\n",
    "                    device, optimizer, criterion,\n",
    "                    log_freq = 5000,\n",
    "                    model_fname =\"models/temp_model_dnn_cifar.ckpt\",\n",
    "                    verbose=False, logging=True)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python (mns-project)",
   "language": "python",
   "name": "mns-project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
