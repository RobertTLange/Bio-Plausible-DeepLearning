{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNS - Biological Plausible Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import Packages\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "\n",
    "# Import Network Architectures\n",
    "from DNN import DNN, train_dnn_model\n",
    "from CNN import CNN, train_cnn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch Device: Local CPU\n",
      "Deleted Old Existing Log Directory\n"
     ]
    }
   ],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Torch Device: {}\".format(torch.cuda.get_device_name(0)))\n",
    "else:\n",
    "    print(\"Torch Device: Local CPU\")\n",
    "    \n",
    "# Remove files in log dir\n",
    "if not os.path.exists(os.getcwd() + \"/logs\"):\n",
    "    os.makedirs(directory)\n",
    "    print(\"Created New Log Directory\")\n",
    "else:\n",
    "    filelist = [ f for f in os.listdir(os.getcwd() + \"/logs\") if f.endswith(\".bak\") ]\n",
    "    for f in filelist:\n",
    "        os.remove(os.path.join(mydir, f))\n",
    "    print(\"Deleted Old Files in Existing Log Directory\")\n",
    "\n",
    "# Define batchsize for data-loading\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# MNIST dataset \n",
    "train_dataset = torchvision.datasets.MNIST(root='../../data', \n",
    "                                           train=True, \n",
    "                                           transform=transforms.ToTensor(),  \n",
    "                                           download=True)\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(root='../../data', \n",
    "                                          train=False, \n",
    "                                          transform=transforms.ToTensor())\n",
    "\n",
    "# Data loader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# CIFAR10 dataset\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='../../data', \n",
    "                                             train=True, \n",
    "                                             transform=transforms.ToTensor(),  \n",
    "                                             download=True)\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='../../data', \n",
    "                                            train=False, \n",
    "                                            transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Feedforward Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=784, out_features=500, bias=True)\n",
      "ReLU()\n",
      "Linear(in_features=500, out_features=10, bias=True)\n"
     ]
    }
   ],
   "source": [
    "# Feedforward Neural Network Parameters\n",
    "h_sizes = [784, 500]\n",
    "out_size = 10\n",
    "num_epochs = 5\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Instantiate the model with layersizes, Loss fct, optimizer\n",
    "dnn_model = DNN(h_sizes, out_size).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(dnn_model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [100/600], Loss: 0.3048, Train Acc: 0.92, Test Acc: 0.91\n",
      "Epoch [1/5], Step [200/600], Loss: 0.2679, Train Acc: 0.93, Test Acc: 0.93\n",
      "Epoch [1/5], Step [300/600], Loss: 0.2407, Train Acc: 0.94, Test Acc: 0.93\n",
      "Epoch [1/5], Step [400/600], Loss: 0.4094, Train Acc: 0.89, Test Acc: 0.94\n",
      "Epoch [1/5], Step [500/600], Loss: 0.1167, Train Acc: 0.97, Test Acc: 0.95\n",
      "Epoch [1/5], Step [600/600], Loss: 0.1576, Train Acc: 0.96, Test Acc: 0.96\n",
      "Epoch [2/5], Step [100/600], Loss: 0.2267, Train Acc: 0.93, Test Acc: 0.96\n",
      "Epoch [2/5], Step [200/600], Loss: 0.1853, Train Acc: 0.91, Test Acc: 0.96\n",
      "Epoch [2/5], Step [300/600], Loss: 0.0985, Train Acc: 0.96, Test Acc: 0.97\n",
      "Epoch [2/5], Step [400/600], Loss: 0.2166, Train Acc: 0.94, Test Acc: 0.97\n",
      "Epoch [2/5], Step [500/600], Loss: 0.0719, Train Acc: 0.98, Test Acc: 0.97\n",
      "Epoch [2/5], Step [600/600], Loss: 0.1051, Train Acc: 0.95, Test Acc: 0.97\n",
      "Epoch [3/5], Step [100/600], Loss: 0.0537, Train Acc: 0.99, Test Acc: 0.97\n",
      "Epoch [3/5], Step [200/600], Loss: 0.0372, Train Acc: 0.99, Test Acc: 0.97\n",
      "Epoch [3/5], Step [300/600], Loss: 0.0430, Train Acc: 0.99, Test Acc: 0.97\n",
      "Epoch [3/5], Step [400/600], Loss: 0.1539, Train Acc: 0.96, Test Acc: 0.97\n",
      "Epoch [3/5], Step [500/600], Loss: 0.0467, Train Acc: 1.00, Test Acc: 0.97\n",
      "Epoch [3/5], Step [600/600], Loss: 0.0884, Train Acc: 0.97, Test Acc: 0.97\n",
      "Epoch [4/5], Step [100/600], Loss: 0.0328, Train Acc: 0.99, Test Acc: 0.98\n",
      "Epoch [4/5], Step [200/600], Loss: 0.0277, Train Acc: 1.00, Test Acc: 0.97\n",
      "Epoch [4/5], Step [300/600], Loss: 0.0480, Train Acc: 0.99, Test Acc: 0.98\n",
      "Epoch [4/5], Step [400/600], Loss: 0.0721, Train Acc: 0.99, Test Acc: 0.98\n",
      "Epoch [4/5], Step [500/600], Loss: 0.0388, Train Acc: 0.99, Test Acc: 0.98\n",
      "Epoch [4/5], Step [600/600], Loss: 0.0229, Train Acc: 1.00, Test Acc: 0.97\n",
      "Epoch [5/5], Step [100/600], Loss: 0.0255, Train Acc: 1.00, Test Acc: 0.98\n",
      "Epoch [5/5], Step [200/600], Loss: 0.0325, Train Acc: 1.00, Test Acc: 0.98\n",
      "Epoch [5/5], Step [300/600], Loss: 0.0519, Train Acc: 0.98, Test Acc: 0.98\n",
      "Epoch [5/5], Step [400/600], Loss: 0.0070, Train Acc: 1.00, Test Acc: 0.98\n",
      "Epoch [5/5], Step [500/600], Loss: 0.0326, Train Acc: 0.99, Test Acc: 0.98\n",
      "Epoch [5/5], Step [600/600], Loss: 0.0279, Train Acc: 0.98, Test Acc: 0.98\n"
     ]
    }
   ],
   "source": [
    "train_dnn_model(dnn_model, num_epochs,\n",
    "                train_loader, test_loader,\n",
    "                device, optimizer, criterion,\n",
    "                model_fname =\"models/mnist_dnn.ckpt\",\n",
    "                verbose=True, logging=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "ReLU()\n",
      "MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "ReLU()\n",
      "MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "Linear(in_features=1568, out_features=10, bias=True)\n"
     ]
    }
   ],
   "source": [
    "# ConvNet Parameters\n",
    "ch_sizes = [1, 16, 32]\n",
    "k_sizes = [5, 5]\n",
    "stride = 1\n",
    "padding = 2\n",
    "out_size = 10\n",
    "num_epochs = 5\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Instantiate the model with layersizes, Loss fct, optimizer\n",
    "cnn_model = CNN(ch_sizes, k_sizes,\n",
    "                stride, padding, out_size).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(cnn_model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [100/600], Loss: 0.1264, Train Acc: 0.99, Test Acc: 0.96\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "global name 'update_logger' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-dffff53645a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m                 \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                 \u001b[0mmodel_fname\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m\"models/temp_model_cnn.ckpt\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m                 verbose=True, logging=True)\n\u001b[0m",
      "\u001b[0;32m/Users/rtl/Dropbox/PHD_ECN/COURSES/WiSe_2018_Models_of_Neural_Systems/PROJECT/CODE/CNN.py\u001b[0m in \u001b[0;36mtrain_cnn_model\u001b[0;34m(model, num_epochs, train_loader, test_loader, device, optimizer, criterion, model_fname, verbose, logging)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m                     update_logger(logger, epoch, i, loss, accuracy, model,\n\u001b[0m\u001b[1;32m     95\u001b[0m                                   images, train_loader)\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: global name 'update_logger' is not defined"
     ]
    }
   ],
   "source": [
    "train_cnn_model(cnn_model, num_epochs,\n",
    "                train_loader, test_loader,\n",
    "                device, optimizer, criterion,\n",
    "                model_fname =\"models/temp_model_cnn.ckpt\",\n",
    "                verbose=True, logging=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Guergiev et al (2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python (mns-project)",
   "language": "python",
   "name": "mns-project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
