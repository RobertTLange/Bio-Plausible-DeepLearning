{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNS - Biological Plausible Deep Learning\n",
    "## Run Guergiev et al (2017) Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "The line_profiler extension is already loaded. To reload it, use:\n",
      "  %reload_ext line_profiler\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%load_ext line_profiler\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **General Structure of Network Module**\n",
    "\n",
    "    * Init: Declare layer shapes, data, define spike history buffer, init weights/layers\n",
    "    * init_weights: Loops over all layers and initializes weights\n",
    "       * Hyperparameters\n",
    "            - use_weight_optimization: Desired avg/sd of somatic potentials\n",
    "            - use_feedback_bias: Not in paper - not only synapses Y but also intercept\n",
    "            - use_sparse_feedback: 80% of weights randomly set to 0\n",
    "            - use_broadcast: feedback to all layers comes from output layer\n",
    "        * TODOs\n",
    "            - Don't loop over layers but sample ones a large set\n",
    "            - Add dropout percentage and magnitude increase vars\n",
    "            \n",
    "    * make_weights_symmetric: \n",
    "        - Hyperparameters: Feedback weights = Transposes feedforward weights\n",
    "            - noisy_symmetric_weights: Add N(0, 0.05) noise to data\n",
    "            - use_broadcast: feedback to all layers comes from output layer\n",
    "        - TODOs\n",
    "            - use_broadcast only needed with more than one hidden layer!\n",
    "    * init_layers: Loops over layers and adds them to l object in class\n",
    "    * out_f: Forward phase pass through net - calls out_f of different layers in loop\n",
    "    * out_t: Target phase pass through net - adds target at top layer\n",
    "        - TODOs\n",
    "            - merge out_f and out_t - None vs self.t\n",
    "    * f_phase: Forward phase\n",
    "    * t_phase: Target phase\n",
    "    * train: Loop over epochs and loop over all examples\n",
    "    * test_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Import log-helper/learning plot functions\n",
    "from utils.helpers import *\n",
    "from utils.plotting import *\n",
    "\n",
    "# Guergiev et al (2017) - Segregated Compartments DNN Learning\n",
    "from models.CompDNN import CompDNN, eval_comp_dnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No download of MNIST needed.\n",
      "No download of Fashion-MNIST needed.\n",
      "No download of CIFAR-10 needed.\n"
     ]
    }
   ],
   "source": [
    "# Create all necessary directory if non-existent\n",
    "global data_dir\n",
    "data_dir = os.getcwd() +\"/data\"\n",
    "\n",
    "if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)\n",
    "    print(\"Created New Data Directory\")\n",
    "    \n",
    "download_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 1, 28, 28) (60000,)\n"
     ]
    }
   ],
   "source": [
    "# MNIST dataset\n",
    "X_mnist, y_mnist = get_data(num_samples=70000, dataset=\"mnist\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_mnist, y_mnist,\n",
    "                                                    stratify=y_mnist,\n",
    "                                                    random_state=0,\n",
    "                                                    test_size=1./7)\n",
    "\n",
    "# set training parameters\n",
    "f_etas = (0.21, 0.21)\n",
    "b_etas = None\n",
    "n_epochs = 10\n",
    "\n",
    "n=(500, 10)\n",
    "\n",
    "net = CompDNN(n, X_train, y_train)\n",
    "\n",
    "net.train(f_etas, b_etas, n_epochs,\n",
    "          log_freq=5000, verbose=True, logging=True, dataset=\"mnist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = prep_data_guergiev(X_test, y_test)\n",
    "acc, loss = net.get_test_error(X_test, y_test)\n",
    "print(\"Test Accuracy MNIST: {}\".format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fashion-MNIST dataset\n",
    "X_fashion, y_fashion = get_data(num_samples=70000, dataset=\"fashion\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_fashion, y_fashion,\n",
    "                                                    stratify=y_fashion,\n",
    "                                                    random_state=0,\n",
    "                                                    test_size=1./7)\n",
    "\n",
    "# set training parameters\n",
    "f_etas = (0.21, 0.21)\n",
    "b_etas = None\n",
    "n_epochs = 10\n",
    "\n",
    "n=(500, 10)\n",
    "\n",
    "net = CompDNN(n, X_train, y_train)\n",
    "\n",
    "net.train(f_etas, b_etas, n_epochs,\n",
    "          log_freq=5000, verbose=True, logging=True, dataset=\"fashion\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = prep_data_guergiev(X_test, y_test)\n",
    "acc, loss = net.get_test_error(X_test, y_test)\n",
    "print(\"Test Accuracy Fashion-MNIST: {}\".format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CIFAR-10 dataset\n",
    "X_cifar10, y_cifar10 = get_data(num_samples=70000, dataset=\"cifar10\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_cifar10, y_cifar10,\n",
    "                                                    stratify=y_cifar10,\n",
    "                                                    random_state=0,\n",
    "                                                    test_size=1./7)\n",
    "\n",
    "# set training parameters\n",
    "f_etas = (0.21, 0.21)\n",
    "b_etas = None\n",
    "n_epochs = 10\n",
    "\n",
    "n=(500, 10)\n",
    "net = CompDNN(n, X_train, y_train)\n",
    "\n",
    "net.train(f_etas, b_etas, n_epochs,\n",
    "          log_freq=5000, verbose=True, logging=True, dataset=\"cifar10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = prep_data_guergiev(X_test, y_test)\n",
    "acc, loss = net.get_test_error(X_test, y_test)\n",
    "print(\"Test Accuracy CIFAR-10: {}\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: mnist\n",
      "Learning Rate Weights: (0.21, 0.21)\n",
      "Architecture of Cross-Validated Network:\n",
      "\t Layer 0: 500 Units\n",
      "\t Layer 1: 10 Units\n",
      "Cross-Validation Score Fold 1: 0.9425351388412753\n",
      "Cross-Validation Score Fold 2: 0.9206702952899327\n",
      "Cross-Validation Score Fold 3: 0.9402083065449403\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9344712468920494"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_comp_dnn(\"mnist\", f_etas, b_etas, num_layers=1,\n",
    "              h_l_1=500, h_l_2=0, h_l_3=0, h_l_4=0, h_l_5=0, h_l_6=0,\n",
    "              num_epochs=2, k_fold=3, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Bayesian Optimization (UCB-Acquisition Fct) on DNN\n",
    "hyper_space_comp_dnn = {'batch_size': (10, 500),\n",
    "                        'learning_rate': (0.01, 0.5),\n",
    "                        'num_layers': (1, 6),\n",
    "                        'h_l_1': (30, 500),\n",
    "                        'h_l_2': (30, 500),\n",
    "                        'h_l_3': (30, 500),\n",
    "                        'h_l_4': (30, 500),\n",
    "                        'h_l_5': (30, 500),\n",
    "                        'h_l_6': (30, 500)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_sparse_feedback = False # silence 80% of feedback weights\n",
    "nonspiking_mode = True  # Non-spiking mode (real-valued outputs)\n",
    "use_conductances = True  # use conductances between dendrites and soma\n",
    "use_broadcast = True  # use broadcast - feedback to all layers from output l\n",
    "use_spiking_feedback = True  # use spiking feedback\n",
    "use_spiking_feedforward = True  # use spiking feedforward input\n",
    "use_symmetric_weights = False  # enforce symmetric weights\n",
    "noisy_symmetric_weights = False  # add noise to symmetric weights\n",
    "\n",
    "\"\"\"\n",
    "Neurophysiological Hyperparameters\n",
    "\"\"\"\n",
    "lambda_max = 0.2*dt  # maximum spike rate (spikes per time step)\n",
    "\n",
    "# kernel parameters\n",
    "tau_s = 3.0  # synaptic time constant\n",
    "tau_L = 10.0  # leak time constant\n",
    "\n",
    "# conductance parameters\n",
    "g_B = 0.6  # basal conductance\n",
    "g_A = 0.05 if use_apical_conductance else 0  # apical conductance\n",
    "g_L = 1.0/tau_L  # leak conductance\n",
    "g_D = g_B  # dendritic conductance in output layer\n",
    "\n",
    "# reversal potential parameters\n",
    "E_E = 8  # excitation\n",
    "E_I = -8  # inhibition\n",
    "\n",
    "# steady state constants\n",
    "k_B = g_B/(g_L + g_B + g_A)\n",
    "k_D = g_D/(g_L + g_D)\n",
    "k_I = 1.0/(g_L + g_D)\n",
    "\n",
    "# weight update constants\n",
    "P_hidden = 20.0/lambda_max      # hidden layer error signal scaling factor\n",
    "P_final = 20.0/(lambda_max**2)  # final layer error signal scaling factor"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (BioDL)",
   "language": "python",
   "name": "biodl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
